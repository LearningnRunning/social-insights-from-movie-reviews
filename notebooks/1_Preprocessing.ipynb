{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Library Installation and Imports\n",
    "First, install and import the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "!pip install googletrans==4.0.0-rc1 navertrans nltk\n",
    "\n",
    "import pandas as pd\n",
    "from googletrans import Translator\n",
    "from navertrans import navertrans\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Download the Punkt tokenizer for sentence splitting\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Translation Function\n",
    "This function translates text based on the detected language:\n",
    "\n",
    "If the language is Chinese (Traditional or Simplified), it uses Naver's translator to translate from Chinese to English.\n",
    "If the language is Japanese, it translates from Japanese to English using Naver.\n",
    "For any other language, it first detects the language with Google Translate and then uses Naver to translate to English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_text(text, source_lang=None):\n",
    "    # Initialize Google Translate for language detection\n",
    "    translator = Translator(service_urls=['translate.google.com', 'translate.google.co.kr'])\n",
    "    \n",
    "    # Detect language if not provided\n",
    "    detected_lang = translator.detect(text).lang if not source_lang else source_lang\n",
    "    \n",
    "    # Translate based on detected language\n",
    "    if detected_lang == 'zh-cn' or detected_lang == 'zh-tw':\n",
    "        # Translate Chinese to English\n",
    "        return navertrans.translate(text, src_lan=\"zh-CN\", tar_lan=\"en\")\n",
    "    elif detected_lang == 'ja':\n",
    "        # Translate Japanese to English\n",
    "        return navertrans.translate(text, src_lan=\"ja\", tar_lan=\"en\")\n",
    "    else:\n",
    "        # For other languages, use detected language with Naver to translate to English\n",
    "        return navertrans.translate(text, src_lan=detected_lang, tar_lan=\"en\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Spell-Check Function\n",
    "For spell-checking English text, the TextBlob library is used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "def spell_check(text):\n",
    "    # Perform spell-check on English text\n",
    "    corrected_text = str(TextBlob(text).correct())\n",
    "    return corrected_text\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Sentence Splitting Function\n",
    "This function splits text into individual sentences and stores them in a new DataFrame. It takes the original DataFrame (sentence_df) and the column containing the text to be split (target_column)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sentence(sentence_df, target_column):\n",
    "    # DataFrame to store split sentences\n",
    "    splited_sentence_df = pd.DataFrame()\n",
    "    \n",
    "    for row_idx, row in sentence_df.iterrows():\n",
    "        review = row[target_column]\n",
    "        # Skip rows if text is NaN or not a string\n",
    "        if pd.isna(review) or not isinstance(review, str):\n",
    "            continue\n",
    "            \n",
    "        # Split text into sentences\n",
    "        sentences = sent_tokenize(review)\n",
    "        for sent in sentences:\n",
    "            # Copy original row and add split sentence\n",
    "            row_copy = pd.DataFrame(row.copy()).T\n",
    "            row_copy['Separated' + target_column] = sent\n",
    "            splited_sentence_df = pd.concat([splited_sentence_df, row_copy], ignore_index=True)\n",
    "    \n",
    "    return splited_sentence_df\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Complete Code Workflow Example\n",
    "To integrate everything, here’s how to process a DataFrame using the functions defined above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data and DataFrame\n",
    "data = {\n",
    "    'Review': ['This sentence is written in Korean.', 'これは日本語で書かれています。', '这是一段中文文本。']\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Apply translation and spell-check\n",
    "df['Translated_Review'] = df['Review'].apply(translate_text)\n",
    "df['SpellChecked_Review'] = df['Translated_Review'].apply(spell_check)\n",
    "\n",
    "# Apply sentence splitting\n",
    "splitted_df = split_sentence(df, 'SpellChecked_Review')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation of the Workflow:\n",
    "Translation: translate_text translates the text in the Review column based on the detected language.\n",
    "Spell-Check: spell_check corrects any English spelling errors in the translated text.\n",
    "Sentence Splitting: split_sentence divides the spell-checked English text into individual sentences and stores them in splitted_df.\n",
    "This structure allows for efficient, step-by-step processing of multilingual text data, making it easier to handle translations, corrections, and sentence-level analysis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "backend_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.1 (main, Sep 23 2024, 07:24:12) [GCC 9.4.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "015de5f27e5491122dd982a8c877ea26fd7f08299a68b0965bb51a9f83c662df"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
